{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['paper', 'rock', 'scissors']\n",
      "Epoch 1/20, Loss: 1.1057\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 102\u001b[0m\n\u001b[0;32m     99\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msave_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    101\u001b[0m \u001b[38;5;66;03m# 모델 학습\u001b[39;00m\n\u001b[1;32m--> 102\u001b[0m train_model(model, train_loader, criterion, optimizer, num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m)\n\u001b[0;32m    104\u001b[0m \u001b[38;5;66;03m# 모델 평가\u001b[39;00m\n\u001b[0;32m    105\u001b[0m evaluate_model(model, test_loader)\n",
      "Cell \u001b[1;32mIn[7], line 71\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, criterion, optimizer, num_epochs)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m     70\u001b[0m     running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m---> 71\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m     72\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     73\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m model(images)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torchvision\\datasets\\folder.py:229\u001b[0m, in \u001b[0;36mDatasetFolder.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;124;03m    index (int): Index\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;124;03m    tuple: (sample, target) where target is class_index of the target class.\u001b[39;00m\n\u001b[0;32m    227\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    228\u001b[0m path, target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples[index]\n\u001b[1;32m--> 229\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader(path)\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    231\u001b[0m     sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(sample)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torchvision\\datasets\\folder.py:268\u001b[0m, in \u001b[0;36mdefault_loader\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    266\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m accimage_loader(path)\n\u001b[0;32m    267\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 268\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pil_loader(path)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torchvision\\datasets\\folder.py:248\u001b[0m, in \u001b[0;36mpil_loader\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    247\u001b[0m     img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(f)\n\u001b[1;32m--> 248\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\PIL\\Image.py:937\u001b[0m, in \u001b[0;36mImage.convert\u001b[1;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[0;32m    889\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert\u001b[39m(\n\u001b[0;32m    890\u001b[0m     \u001b[38;5;28mself\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, matrix\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dither\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, palette\u001b[38;5;241m=\u001b[39mPalette\u001b[38;5;241m.\u001b[39mWEB, colors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m\n\u001b[0;32m    891\u001b[0m ):\n\u001b[0;32m    892\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    893\u001b[0m \u001b[38;5;124;03m    Returns a converted copy of this image. For the \"P\" mode, this\u001b[39;00m\n\u001b[0;32m    894\u001b[0m \u001b[38;5;124;03m    method translates pixels through the palette.  If mode is\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    934\u001b[0m \u001b[38;5;124;03m    :returns: An :py:class:`~PIL.Image.Image` object.\u001b[39;00m\n\u001b[0;32m    935\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 937\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload()\n\u001b[0;32m    939\u001b[0m     has_transparency \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransparency\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    940\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    941\u001b[0m         \u001b[38;5;66;03m# determine default mode\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\PIL\\ImageFile.py:269\u001b[0m, in \u001b[0;36mImageFile.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    266\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[0;32m    268\u001b[0m b \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m+\u001b[39m s\n\u001b[1;32m--> 269\u001b[0m n, err_code \u001b[38;5;241m=\u001b[39m decoder\u001b[38;5;241m.\u001b[39mdecode(b)\n\u001b[0;32m    270\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    271\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 1. 경로 설정\n",
    "data_dir = './Rock-Paper-Scissors'\n",
    "train_dir = os.path.join(data_dir, 'train2')\n",
    "test_dir = os.path.join(data_dir, 'test')\n",
    "\n",
    "# 2. 데이터 전처리\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),  # 모든 이미지를 64x64로 크기 조정\n",
    "    transforms.ToTensor(),        # 텐서로 변환\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])  # 정규화\n",
    "])\n",
    "\n",
    "# 3. 데이터셋 로드\n",
    "train_dataset = datasets.ImageFolder(root=train_dir, transform=transform)\n",
    "test_dataset = datasets.ImageFolder(root=test_dir, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=25, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=25, shuffle=False)\n",
    "\n",
    "# 클래스 이름 확인\n",
    "classes = train_dataset.classes\n",
    "print(f\"Classes: {classes}\")\n",
    "\n",
    "# 4. CNN 모델 정의\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),  # 입력 채널: 3, 출력 채널: 32\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # 64x64 -> 32x32\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),  # 입력 채널: 32, 출력 채널: 64\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # 32x32 -> 16x16\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),  # 입력 채널: 64, 출력 채널: 128\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)  # 16x16 -> 8x8\n",
    "        )\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Flatten(),               # 128x8x8 -> 8192\n",
    "            nn.Linear(128 * 8 * 8, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 3)            # 최종 출력: 3 (rock, paper, scissors)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n",
    "\n",
    "model = SimpleCNN()\n",
    "\n",
    "# 5. 손실 함수와 옵티마이저 정의\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 6. 학습 루프\n",
    "def train_model(model, train_loader, criterion, optimizer, num_epochs=20):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader):.4f}\")\n",
    "\n",
    "# 7. 평가 루프\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "# 8. 모델 저장 함수 추가\n",
    "def save_model(model, save_path):\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    print(f\"Model saved to {save_path}\")\n",
    "\n",
    "# 모델 학습\n",
    "train_model(model, train_loader, criterion, optimizer, num_epochs=20)\n",
    "\n",
    "# 모델 평가\n",
    "evaluate_model(model, test_loader)\n",
    "\n",
    "# 학습한 모델 저장\n",
    "save_path = './rock_paper_scissors_model.pth'  # 저장 파일 경로\n",
    "save_model(model, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# train_dataset에서 임의의 데이터 가져오기\n",
    "def show_sample_image(dataset):\n",
    "    image, label = dataset[0]  # 첫 번째 이미지를 불러옵니다 (인덱스를 바꿔 임의의 이미지 확인 가능)\n",
    "    \n",
    "    # 이미지를 정규화에서 원래대로 되돌리기\n",
    "    image = image.numpy().transpose((1, 2, 0))  # CxHxW -> HxWxC로 변환\n",
    "    image = image * 0.5 + 0.5  # Normalize(mean=[0.5], std=[0.5])의 반전\n",
    "    \n",
    "    # 시각화\n",
    "    plt.imshow(image)\n",
    "    plt.title(f\"Label: {train_dataset.classes[label]}\")  # 레이블의 이름 표시\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# 함수 호출\n",
    "show_sample_image(train_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "실시간 분류 시작: 'q'를 눌러 종료하세요.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# 1. 데이터 전처리 함수 정의\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # 모델 입력 크기와 맞춤\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# 2. 학습된 모델 불러오기\n",
    "# 모델 클래스 정의 (학습 시 사용한 CNN 클래스와 동일해야 함)\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128 * 8 * 8, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 3)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n",
    "\n",
    "# 저장된 모델 불러오기\n",
    "model = SimpleCNN()\n",
    "model.load_state_dict(torch.load('./rock_paper_scissors_model.pth'))  # 학습된 모델 경로\n",
    "model.eval()\n",
    "\n",
    "# 클래스 이름 설정\n",
    "classes = [\"paper\", \"rock\", \"scissors\"]\n",
    "\n",
    "# 3. 카메라 열기\n",
    "cap = cv2.VideoCapture(0)  # 디바이스 기본 카메라\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"카메라를 열 수 없습니다.\")\n",
    "    exit()\n",
    "\n",
    "print(\"실시간 분류 시작: 'q'를 눌러 종료하세요.\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"카메라로부터 프레임을 가져올 수 없습니다.\")\n",
    "        break\n",
    "\n",
    "    # 4. OpenCV 이미지 전처리\n",
    "    # BGR(OpenCV) -> RGB(Pillow) 변환\n",
    "    img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    img_pil = Image.fromarray(img_rgb)\n",
    "\n",
    "    # 모델 입력을 위한 전처리\n",
    "    input_tensor = transform(img_pil).unsqueeze(0)  # 배치 차원 추가 (1, C, H, W)\n",
    "\n",
    "    # 5. 모델 예측\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        label = classes[predicted.item()]\n",
    "\n",
    "    # 6. 결과 화면에 표시\n",
    "    cv2.putText(frame, f\"Prediction: {label}\", (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "    cv2.imshow(\"Rock-Paper-Scissors Classification\", frame)\n",
    "\n",
    "    # 'q'를 누르면 종료\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# 7. 리소스 해제\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['paper', 'rock', 'scissors']\n",
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ResNetForImageClassification were not initialized from the model checkpoint at microsoft/resnet-50 and are newly initialized because the shapes did not match:\n",
      "- classifier.1.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([3]) in the model instantiated\n",
      "- classifier.1.weight: found shape torch.Size([1000, 2048]) in the checkpoint and torch.Size([3, 2048]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Loss: 0.7896\n",
      "Epoch 2/30, Loss: 0.1383\n",
      "Epoch 3/30, Loss: 0.0352\n",
      "Epoch 4/30, Loss: 0.0158\n",
      "Epoch 5/30, Loss: 0.0074\n",
      "Epoch 6/30, Loss: 0.0050\n",
      "Epoch 7/30, Loss: 0.0052\n",
      "Epoch 8/30, Loss: 0.0056\n",
      "Epoch 9/30, Loss: 0.0044\n",
      "Epoch 10/30, Loss: 0.0062\n",
      "Epoch 11/30, Loss: 0.0099\n",
      "Epoch 12/30, Loss: 0.0067\n",
      "Epoch 13/30, Loss: 0.0051\n",
      "Epoch 14/30, Loss: 0.0033\n",
      "Epoch 15/30, Loss: 0.0021\n",
      "Epoch 16/30, Loss: 0.0044\n",
      "Epoch 17/30, Loss: 0.0267\n",
      "Epoch 18/30, Loss: 0.0035\n",
      "Epoch 19/30, Loss: 0.0015\n",
      "Epoch 20/30, Loss: 0.0011\n",
      "Epoch 21/30, Loss: 0.0010\n",
      "Epoch 22/30, Loss: 0.0015\n",
      "Epoch 23/30, Loss: 0.0012\n",
      "Epoch 24/30, Loss: 0.0102\n",
      "Epoch 25/30, Loss: 0.0099\n",
      "Epoch 26/30, Loss: 0.0021\n",
      "Epoch 27/30, Loss: 0.0008\n",
      "Epoch 28/30, Loss: 0.0192\n",
      "Epoch 29/30, Loss: 0.0028\n",
      "Epoch 30/30, Loss: 0.0008\n",
      "Accuracy: 98.92%\n",
      "Model saved to ./rock_paper_scissors_pretrained_model.pth\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
    "\n",
    "# 1. 경로 설정\n",
    "data_dir = './Rock-Paper-Scissors'\n",
    "train_dir = os.path.join(data_dir, 'train3')\n",
    "test_dir = os.path.join(data_dir, 'test')\n",
    "\n",
    "# 2. 데이터 전처리\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Pretrained 모델에 맞게 크기 조정\n",
    "    transforms.ToTensor(),          # 텐서로 변환\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # 3채널 정규화\n",
    "])\n",
    "\n",
    "# 3. 데이터셋 로드\n",
    "train_dataset = datasets.ImageFolder(root=train_dir, transform=transform)\n",
    "test_dataset = datasets.ImageFolder(root=test_dir, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# 클래스 이름 확인\n",
    "classes = train_dataset.classes\n",
    "print(f\"Classes: {classes}\")\n",
    "\n",
    "# 4. CUDA 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 5. Pretrained 모델 불러오기\n",
    "pretrained_model_name = \"microsoft/resnet-50\"\n",
    "model = AutoModelForImageClassification.from_pretrained(\n",
    "    pretrained_model_name,\n",
    "    num_labels=3,  # 가위바위보 3개 클래스로 분류\n",
    "    ignore_mismatched_sizes=True  # Pretrained 모델과 출력 레이어 크기가 다를 때 강제로 맞춤\n",
    ")\n",
    "\n",
    "# Feature Extractor 출력 크기 확인 및 분류기 수정\n",
    "model.classifier = nn.Sequential(\n",
    "    nn.Flatten(),  # Feature Map을 1D 벡터로 변환\n",
    "    nn.Linear(2048, 3)  # ResNet Feature Extractor의 출력 크기와 맞춤\n",
    ")\n",
    "\n",
    "# 모델을 GPU 또는 CPU로 이동\n",
    "model = model.to(device)\n",
    "\n",
    "# 6. 손실 함수와 옵티마이저 정의\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# 7. 학습 루프\n",
    "def train_model(model, train_loader, criterion, optimizer, num_epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)  # 데이터를 GPU로 전송\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)  # Forward pass\n",
    "            loss = criterion(outputs.logits, labels)  # Loss 계산 (logits 사용)\n",
    "            loss.backward()  # Backward pass\n",
    "            optimizer.step()  # Optimizer 업데이트\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader):.4f}\")\n",
    "\n",
    "# 8. 평가 루프\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)  # 데이터를 GPU로 전송\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.logits, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "# 9. 모델 저장 함수 추가\n",
    "def save_model(model, save_path):\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    print(f\"Model saved to {save_path}\")\n",
    "\n",
    "# 10. 모델 학습\n",
    "train_model(model, train_loader, criterion, optimizer, num_epochs=30)\n",
    "\n",
    "# 11. 모델 평가\n",
    "evaluate_model(model, test_loader)\n",
    "\n",
    "# 12. 학습한 모델 저장\n",
    "save_path = './rock_paper_scissors_pretrained_model.pth'\n",
    "save_model(model, save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ResNetForImageClassification were not initialized from the model checkpoint at microsoft/resnet-50 and are newly initialized because the shapes did not match:\n",
      "- classifier.1.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([3]) in the model instantiated\n",
      "- classifier.1.weight: found shape torch.Size([1000, 2048]) in the checkpoint and torch.Size([3, 2048]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press 'q' to exit.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from transformers import AutoModelForImageClassification\n",
    "\n",
    "# 1. 모델 불러오기\n",
    "model_path = './rock_paper_scissors_pretrained_model.pth'\n",
    "pretrained_model_name = \"microsoft/resnet-50\"\n",
    "\n",
    "# Pretrained 모델 로드\n",
    "model = AutoModelForImageClassification.from_pretrained(\n",
    "    pretrained_model_name,\n",
    "    num_labels=3,\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "model.classifier = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(2048, 3)  # 가위, 바위, 보\n",
    ")\n",
    "\n",
    "# 모델 가중치 불러오기\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()\n",
    "\n",
    "# CUDA 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# 클래스 이름\n",
    "classes = ['paper', 'rock', 'scissors']\n",
    "\n",
    "# 2. 이미지 전처리 함수\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),  # 모델 입력 크기에 맞게 조정\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # 학습 시 정규화와 동일\n",
    "])\n",
    "\n",
    "# 3. 실시간 카메라 캡처\n",
    "cap = cv2.VideoCapture(0)  # 웹캠 열기\n",
    "if not cap.isOpened():\n",
    "    print(\"Could not open webcam.\")\n",
    "    exit()\n",
    "\n",
    "print(\"Press 'q' to exit.\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to capture frame.\")\n",
    "        break\n",
    "\n",
    "    # 화면에 프레임 출력\n",
    "    cv2.imshow(\"Rock-Paper-Scissors Classifier\", frame)\n",
    "\n",
    "    # 프레임 중앙 부분을 잘라내기 (옵션)\n",
    "    h, w, _ = frame.shape\n",
    "    min_dim = min(h, w)\n",
    "    crop_img = frame[(h - min_dim) // 2:(h + min_dim) // 2, (w - min_dim) // 2:(w + min_dim) // 2]\n",
    "\n",
    "    # 이미지 전처리\n",
    "    input_image = transform(crop_img).unsqueeze(0).to(device)\n",
    "\n",
    "    # 모델 예측\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_image)\n",
    "        _, predicted = torch.max(outputs.logits, 1)\n",
    "        predicted_class = classes[predicted.item()]\n",
    "\n",
    "    # 예측 결과 출력\n",
    "    cv2.putText(frame, f\"Prediction: {predicted_class}\", (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    # 화면에 표시\n",
    "    cv2.imshow(\"Rock-Paper-Scissors Classifier\", frame)\n",
    "\n",
    "    # 'q' 키를 누르면 종료\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# 카메라 및 창 닫기\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
